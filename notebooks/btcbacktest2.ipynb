{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ab2693-a5d6-4d29-b8f8-471d362e3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 18:53:35,253 - ERROR - Failed to connect to TimescaleDB: connection to server at \"timescaledb\" (172.20.0.2), port 5432 failed: FATAL:  sorry, too many clients already\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getconn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1129\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1127\u001b[0m     config \u001b[38;5;241m=\u001b[39m BacktestConfig()\n\u001b[0;32m-> 1129\u001b[0m     collector \u001b[38;5;241m=\u001b[39m \u001b[43mHistoricalDataCollector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALPACA_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_secret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALPACA_API_SECRET\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFVG Fade Strategy Backtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 305\u001b[0m, in \u001b[0;36mHistoricalDataCollector.__init__\u001b[0;34m(self, db, api_key, api_secret)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m db\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_client \u001b[38;5;241m=\u001b[39m CryptoHistoricalDataClient(api_key, api_secret)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_ohlcv_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 309\u001b[0m, in \u001b[0;36mHistoricalDataCollector._create_ohlcv_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_ohlcv_table\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create OHLCV table in TimescaleDB\"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetconn\u001b[49m()\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getconn'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BTC Backtesting Script - FVG Fade Strategy (Converted from PineScript)\n",
    "Implements Fair Value Gap detection with Fade and Inside FVG signals\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "\n",
    "# Modern Alpaca imports\n",
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.data.historical.crypto import CryptoHistoricalDataClient\n",
    "from alpaca.data.requests import CryptoBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "\n",
    "# Observability imports\n",
    "from prometheus_client import Counter, Gauge, start_http_server, REGISTRY\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Loki logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "# Database (TimescaleDB)\n",
    "import psycopg2\n",
    "from psycopg2.pool import SimpleConnectionPool\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "\n",
    "# Configure logging\n",
    "class LokiHandler(logging.Handler):\n",
    "    \"\"\"Custom handler to send logs to Loki\"\"\"\n",
    "    def __init__(self, loki_url, labels=None):\n",
    "        super().__init__()\n",
    "        self.loki_url = loki_url\n",
    "        self.labels = labels or {}\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            log_entry = {\n",
    "                \"streams\": [{\n",
    "                    \"stream\": {\n",
    "                        \"job\": \"btc_backtest_fvg\",\n",
    "                        \"level\": record.levelname.lower(),\n",
    "                        **self.labels\n",
    "                    },\n",
    "                    \"values\": [[\n",
    "                        str(int(record.created * 1e9)),\n",
    "                        json.dumps({\n",
    "                            \"message\": self.format(record),\n",
    "                            \"level\": record.levelname,\n",
    "                            \"logger\": record.name\n",
    "                        })\n",
    "                    ]]\n",
    "                }]\n",
    "            }\n",
    "            response = self.session.post(self.loki_url, json=log_entry, timeout=5)\n",
    "            response.raise_for_status()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def setup_logging(level=logging.INFO, loki_url=None, loki_labels=None):\n",
    "    \"\"\"Setup logging\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "    \n",
    "    if loki_url:\n",
    "        try:\n",
    "            loki_handler = LokiHandler(loki_url, loki_labels)\n",
    "            loki_handler.setFormatter(formatter)\n",
    "            logger.addHandler(loki_handler)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# FVG Data Class\n",
    "@dataclass\n",
    "class FVG:\n",
    "    \"\"\"Fair Value Gap data structure\"\"\"\n",
    "    max_price: float\n",
    "    min_price: float\n",
    "    is_bullish: bool\n",
    "    timestamp: pd.Timestamp\n",
    "    bar_index: int\n",
    "    \n",
    "    def is_mitigated(self, current_price: float) -> bool:\n",
    "        \"\"\"Check if FVG is mitigated\"\"\"\n",
    "        if self.is_bullish:\n",
    "            return current_price < self.min_price\n",
    "        else:\n",
    "            return current_price > self.max_price\n",
    "    \n",
    "    def is_inside(self, price: float) -> bool:\n",
    "        \"\"\"Check if price is inside the FVG\"\"\"\n",
    "        return self.min_price < price < self.max_price\n",
    "\n",
    "# Database connection pool for TimescaleDB\n",
    "class TimescaleDB:\n",
    "    \"\"\"TimescaleDB connection and operations\"\"\"\n",
    "    def __init__(self, connection_string=None):\n",
    "        self.connection_string = connection_string\n",
    "        self.pool = None\n",
    "        if connection_string:\n",
    "            try:\n",
    "                self.pool = SimpleConnectionPool(1, 10, connection_string)\n",
    "                self._init_schema()\n",
    "                logger.info(\"TimescaleDB connection pool created\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to connect to TimescaleDB: {e}\")\n",
    "    \n",
    "    def _init_schema(self):\n",
    "        \"\"\"Initialize database schema\"\"\"\n",
    "        conn = self.pool.getconn()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            \n",
    "            # Create trades table\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS trades (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "                    symbol VARCHAR(50) NOT NULL,\n",
    "                    direction VARCHAR(10) NOT NULL,\n",
    "                    quantity DECIMAL(18, 8) NOT NULL,\n",
    "                    entry_price DECIMAL(18, 2) NOT NULL,\n",
    "                    stop_loss DECIMAL(18, 2),\n",
    "                    take_profit DECIMAL(18, 2),\n",
    "                    atr_value DECIMAL(18, 2),\n",
    "                    status VARCHAR(20) DEFAULT 'open',\n",
    "                    exit_price DECIMAL(18, 2),\n",
    "                    exit_timestamp TIMESTAMPTZ,\n",
    "                    pnl DECIMAL(18, 2),\n",
    "                    daily_trade_number INTEGER,\n",
    "                    mlflow_run_id VARCHAR(255),\n",
    "                    signal_type VARCHAR(50)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create backtest_results table\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS backtest_results (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "                    start_date DATE NOT NULL,\n",
    "                    end_date DATE NOT NULL,\n",
    "                    symbol VARCHAR(50) NOT NULL,\n",
    "                    timeframe VARCHAR(10) NOT NULL,\n",
    "                    total_trades INTEGER,\n",
    "                    winning_trades INTEGER,\n",
    "                    losing_trades INTEGER,\n",
    "                    win_rate DECIMAL(5, 4),\n",
    "                    total_return_pct DECIMAL(10, 4),\n",
    "                    roi DECIMAL(10, 4),\n",
    "                    sharpe_ratio DECIMAL(10, 4),\n",
    "                    max_drawdown_pct DECIMAL(10, 4),\n",
    "                    profit_factor DECIMAL(10, 4),\n",
    "                    avg_win_pct DECIMAL(10, 4),\n",
    "                    avg_loss_pct DECIMAL(10, 4),\n",
    "                    initial_capital DECIMAL(18, 2),\n",
    "                    final_equity DECIMAL(18, 2),\n",
    "                    net_profit DECIMAL(18, 2),\n",
    "                    mlflow_run_id VARCHAR(255)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            try:\n",
    "                cur.execute(\"SELECT create_hypertable('trades', 'timestamp', if_not_exists => TRUE);\")\n",
    "                cur.execute(\"SELECT create_hypertable('backtest_results', 'timestamp', if_not_exists => TRUE);\")\n",
    "            except Exception:\n",
    "                logger.warning(\"Could not create hypertable (may be regular PostgreSQL)\")\n",
    "            \n",
    "            # Add missing columns if table already exists (migration)\n",
    "            try:\n",
    "                # Check if signal_type column exists\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT column_name \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_name='trades' AND column_name='signal_type';\n",
    "                \"\"\")\n",
    "                if cur.fetchone() is None:\n",
    "                    cur.execute(\"ALTER TABLE trades ADD COLUMN signal_type VARCHAR(50);\")\n",
    "                    logger.info(\"Added signal_type column to trades table\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not add signal_type column: {e}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            logger.info(\"TimescaleDB schema initialized\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.warning(f\"Schema initialization warning: {e}\")\n",
    "        finally:\n",
    "            self.pool.putconn(conn)\n",
    "    \n",
    "    def insert_backtest_results(self, metrics_data):\n",
    "        \"\"\"Insert backtest metrics into database\"\"\"\n",
    "        if not self.pool:\n",
    "            return None\n",
    "        \n",
    "        conn = self.pool.getconn()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO backtest_results (\n",
    "                    start_date, end_date, symbol, timeframe,\n",
    "                    total_trades, winning_trades, losing_trades, win_rate,\n",
    "                    total_return_pct, roi, sharpe_ratio, max_drawdown_pct,\n",
    "                    profit_factor, avg_win_pct, avg_loss_pct,\n",
    "                    initial_capital, final_equity, net_profit, mlflow_run_id\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                RETURNING id;\n",
    "            \"\"\", (\n",
    "                metrics_data.get('start_date'),\n",
    "                metrics_data.get('end_date'),\n",
    "                metrics_data.get('symbol'),\n",
    "                metrics_data.get('timeframe'),\n",
    "                metrics_data.get('total_trades'),\n",
    "                metrics_data.get('winning_trades'),\n",
    "                metrics_data.get('losing_trades'),\n",
    "                metrics_data.get('win_rate'),\n",
    "                metrics_data.get('total_return_pct'),\n",
    "                metrics_data.get('roi'),\n",
    "                metrics_data.get('sharpe_ratio'),\n",
    "                metrics_data.get('max_drawdown_pct'),\n",
    "                metrics_data.get('profit_factor'),\n",
    "                metrics_data.get('avg_win_pct'),\n",
    "                metrics_data.get('avg_loss_pct'),\n",
    "                metrics_data.get('initial_capital'),\n",
    "                metrics_data.get('final_equity'),\n",
    "                metrics_data.get('net_profit'),\n",
    "                metrics_data.get('mlflow_run_id')\n",
    "            ))\n",
    "            result_id = cur.fetchone()[0]\n",
    "            conn.commit()\n",
    "            return result_id\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error inserting backtest results: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            self.pool.putconn(conn)\n",
    "    \n",
    "    def insert_trade(self, trade_data):\n",
    "        \"\"\"Insert a trade record\"\"\"\n",
    "        if not self.pool:\n",
    "            return None\n",
    "        \n",
    "        conn = self.pool.getconn()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO trades (\n",
    "                    symbol, direction, quantity, entry_price, stop_loss, \n",
    "                    take_profit, atr_value, daily_trade_number, mlflow_run_id, signal_type\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                RETURNING id;\n",
    "            \"\"\", (\n",
    "                trade_data.get('symbol'),\n",
    "                trade_data.get('direction'),\n",
    "                trade_data.get('quantity'),\n",
    "                trade_data.get('entry_price'),\n",
    "                trade_data.get('stop_loss'),\n",
    "                trade_data.get('take_profit'),\n",
    "                trade_data.get('atr_value'),\n",
    "                trade_data.get('daily_trade_number'),\n",
    "                trade_data.get('mlflow_run_id'),\n",
    "                trade_data.get('signal_type', 'unknown')\n",
    "            ))\n",
    "            trade_id = cur.fetchone()[0]\n",
    "            conn.commit()\n",
    "            return trade_id\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error inserting trade: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            self.pool.putconn(conn)\n",
    "\n",
    "# Historical Data Collector (reuse from original)\n",
    "class HistoricalDataCollector:\n",
    "    \"\"\"Collect and store historical BTC data in TimescaleDB\"\"\"\n",
    "    \n",
    "    def __init__(self, db: TimescaleDB, api_key: str, api_secret: str):\n",
    "        self.db = db\n",
    "        self.data_client = CryptoHistoricalDataClient(api_key, api_secret)\n",
    "        self._create_ohlcv_table()\n",
    "    \n",
    "    def _create_ohlcv_table(self):\n",
    "        \"\"\"Create OHLCV table in TimescaleDB\"\"\"\n",
    "        conn = self.db.pool.getconn()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS ohlcv_data (\n",
    "                    timestamp TIMESTAMPTZ NOT NULL,\n",
    "                    symbol VARCHAR(20) NOT NULL,\n",
    "                    timeframe VARCHAR(10) NOT NULL,\n",
    "                    open DECIMAL(20, 8),\n",
    "                    high DECIMAL(20, 8),\n",
    "                    low DECIMAL(20, 8),\n",
    "                    close DECIMAL(20, 8),\n",
    "                    volume DECIMAL(30, 8),\n",
    "                    PRIMARY KEY (timestamp, symbol, timeframe)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            try:\n",
    "                cur.execute(\"SELECT create_hypertable('ohlcv_data', 'timestamp', if_not_exists => TRUE);\")\n",
    "            except Exception:\n",
    "                logger.warning(\"Could not create hypertable for ohlcv_data\")\n",
    "            \n",
    "            cur.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS idx_ohlcv_symbol_timeframe \n",
    "                ON ohlcv_data (symbol, timeframe, timestamp DESC);\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.commit()\n",
    "            logger.info(\"OHLCV table created/verified\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error creating OHLCV table: {e}\")\n",
    "        finally:\n",
    "            self.db.pool.putconn(conn)\n",
    "    \n",
    "    def _normalize_symbol(self, symbol: str) -> str:\n",
    "        \"\"\"Normalize symbol format (BTC/USD -> BTCUSD)\"\"\"\n",
    "        return symbol.replace('/', '').replace('-', '').upper()\n",
    "    \n",
    "    def get_historical_data_from_db(self, symbol: str, start_date: str, \n",
    "                                    end_date: str, timeframe: str = '5Min') -> pd.DataFrame:\n",
    "        \"\"\"Retrieve historical data from TimescaleDB\"\"\"\n",
    "        conn = self.db.pool.getconn()\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT DISTINCT symbol FROM ohlcv_data LIMIT 10;\")\n",
    "            existing_symbols = [row[0] for row in cur.fetchall()]\n",
    "            \n",
    "            normalized_symbol = self._normalize_symbol(symbol)\n",
    "            symbol_to_use = None\n",
    "            \n",
    "            if normalized_symbol in existing_symbols:\n",
    "                symbol_to_use = normalized_symbol\n",
    "            elif symbol in existing_symbols:\n",
    "                symbol_to_use = symbol\n",
    "            else:\n",
    "                for db_symbol in existing_symbols:\n",
    "                    if self._normalize_symbol(db_symbol) == normalized_symbol:\n",
    "                        symbol_to_use = db_symbol\n",
    "                        break\n",
    "            \n",
    "            if symbol_to_use is None:\n",
    "                logger.warning(f\"No matching symbol found for {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            start_dt = pd.to_datetime(start_date)\n",
    "            if start_dt.tz is None:\n",
    "                start_dt = start_dt.tz_localize('UTC')\n",
    "            else:\n",
    "                start_dt = start_dt.tz_convert('UTC')\n",
    "            \n",
    "            end_dt = pd.to_datetime(end_date)\n",
    "            if end_dt.tz is None:\n",
    "                end_dt = end_dt.tz_localize('UTC')\n",
    "            else:\n",
    "                end_dt = end_dt.tz_convert('UTC')\n",
    "            end_dt = end_dt + pd.Timedelta(days=1)\n",
    "            \n",
    "            query = \"\"\"\n",
    "                SELECT timestamp, open, high, low, close, volume\n",
    "                FROM ohlcv_data\n",
    "                WHERE symbol = %s \n",
    "                AND timeframe = %s\n",
    "                AND timestamp >= %s \n",
    "                AND timestamp < %s\n",
    "                ORDER BY timestamp ASC;\n",
    "            \"\"\"\n",
    "            cur.execute(query, (symbol_to_use, timeframe, start_dt, end_dt))\n",
    "            rows = cur.fetchall()\n",
    "            colnames = [desc[0] for desc in cur.description]\n",
    "            cur.close()\n",
    "            \n",
    "            if len(rows) == 0:\n",
    "                return None\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=colnames)\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            \n",
    "            # Convert Decimal columns to float (PostgreSQL DECIMAL returns as Decimal type)\n",
    "            numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(float)\n",
    "            \n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            logger.info(f\"Retrieved {len(df)} bars from database for {symbol_to_use}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving data from database: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            self.db.pool.putconn(conn)\n",
    "\n",
    "# FVG Fade Strategy Bot\n",
    "class FVGFadeBacktestBot:\n",
    "    \"\"\"Backtest bot implementing FVG Fade strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, db: TimescaleDB, data_collector: HistoricalDataCollector,\n",
    "                 symbol='BTCUSD', quantity=0.001, initial_capital=10000.0,\n",
    "                 # FVG Settings\n",
    "                 fvg_threshold_pct=0.0, fvg_auto=False, fvg_proximity_ticks=50,\n",
    "                 # Fade Settings\n",
    "                 fade_momentum_threshold=1.1, fade_mom_lookback=20,\n",
    "                 fade_max_momentum_age_mins=120, fade_proximity_ticks=25,\n",
    "                 # Trade Management\n",
    "                 stop_loss_ticks=100, take_profit_ticks=100,\n",
    "                 use_atr_exits=False, atr_period=14,\n",
    "                 atr_stop_loss_mult=1.5, atr_take_profit_mult=3.0,\n",
    "                 # Filters\n",
    "                 require_volume_surge=False, vol_lookback=20, min_volume_ratio=1.2,\n",
    "                 use_atr_volatility_filter=False, atr_vol_filter_period=20,\n",
    "                 atr_vol_filter_ma_period=50, atr_vol_filter_min=0.7, atr_vol_filter_max=3.0,\n",
    "                 use_volume_confirmation=False, volume_filter_ma_period=20,\n",
    "                 volume_filter_multiplier=1.0,\n",
    "                 # FVG Signal Requirements\n",
    "                 require_fvg_confirmation=True, fvg_signal_type=\"Same Direction\",\n",
    "                 inside_fvg_signals=True, inside_fvg_max_bars=20,\n",
    "                 # Liquidity Levels\n",
    "                 keep_levels_days=5, tick_size=0.01):\n",
    "        \n",
    "        self.db = db\n",
    "        self.data_collector = data_collector\n",
    "        self.symbol = symbol\n",
    "        self.quantity = quantity\n",
    "        self.initial_capital = initial_capital\n",
    "        self.current_capital = initial_capital\n",
    "        \n",
    "        # FVG Settings\n",
    "        self.fvg_threshold_pct = fvg_threshold_pct / 100.0\n",
    "        self.fvg_auto = fvg_auto\n",
    "        self.fvg_proximity_ticks = fvg_proximity_ticks\n",
    "        self.tick_size = tick_size\n",
    "        \n",
    "        # Fade Settings\n",
    "        self.fade_momentum_threshold = fade_momentum_threshold\n",
    "        self.fade_mom_lookback = fade_mom_lookback\n",
    "        self.fade_max_momentum_age_mins = fade_max_momentum_age_mins\n",
    "        self.fade_proximity_ticks = fade_proximity_ticks\n",
    "        \n",
    "        # Trade Management\n",
    "        self.stop_loss_ticks = stop_loss_ticks\n",
    "        self.take_profit_ticks = take_profit_ticks\n",
    "        self.use_atr_exits = use_atr_exits\n",
    "        self.atr_period = atr_period\n",
    "        self.atr_stop_loss_mult = atr_stop_loss_mult\n",
    "        self.atr_take_profit_mult = atr_take_profit_mult\n",
    "        \n",
    "        # Filters\n",
    "        self.require_volume_surge = require_volume_surge\n",
    "        self.vol_lookback = vol_lookback\n",
    "        self.min_volume_ratio = min_volume_ratio\n",
    "        self.use_atr_volatility_filter = use_atr_volatility_filter\n",
    "        self.atr_vol_filter_period = atr_vol_filter_period\n",
    "        self.atr_vol_filter_ma_period = atr_vol_filter_ma_period\n",
    "        self.atr_vol_filter_min = atr_vol_filter_min\n",
    "        self.atr_vol_filter_max = atr_vol_filter_max\n",
    "        self.use_volume_confirmation = use_volume_confirmation\n",
    "        self.volume_filter_ma_period = volume_filter_ma_period\n",
    "        self.volume_filter_multiplier = volume_filter_multiplier\n",
    "        \n",
    "        # FVG Signal Requirements\n",
    "        self.require_fvg_confirmation = require_fvg_confirmation\n",
    "        self.fvg_signal_type = fvg_signal_type\n",
    "        self.inside_fvg_signals = inside_fvg_signals\n",
    "        self.inside_fvg_max_bars = inside_fvg_max_bars\n",
    "        \n",
    "        # Liquidity Levels\n",
    "        self.keep_levels_days = keep_levels_days\n",
    "        self.max_levels = keep_levels_days * 2\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.fvg_records: List[FVG] = []\n",
    "        self.liquidity_levels: List[float] = []\n",
    "        self.last_momentum_candle_bar: Optional[int] = None\n",
    "        self.last_momentum_candle_bullish: bool = False\n",
    "        self.last_signal_bar: int = 0\n",
    "        self.yesterday_levels_added: bool = False\n",
    "        self.recent_bull_fvg: bool = False\n",
    "        self.recent_bear_fvg: bool = False\n",
    "        self.recent_bull_fvg_min: Optional[float] = None\n",
    "        self.recent_bull_fvg_max: Optional[float] = None\n",
    "        self.recent_bear_fvg_min: Optional[float] = None\n",
    "        self.recent_bear_fvg_max: Optional[float] = None\n",
    "        self.fvg_detection_bar: int = 0\n",
    "        \n",
    "        # Backtest tracking\n",
    "        self.backtest_results = []\n",
    "        self.positions = []\n",
    "        self.current_mlflow_run = None\n",
    "    \n",
    "    def calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Average True Range\"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close'].shift(1)\n",
    "        \n",
    "        tr1 = high - low\n",
    "        tr2 = abs(high - close)\n",
    "        tr3 = abs(low - close)\n",
    "        \n",
    "        tr = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)\n",
    "        atr = tr.rolling(window=period).mean()\n",
    "        \n",
    "        return atr\n",
    "    \n",
    "    def detect_fvg(self, df: pd.DataFrame, bar_idx: int) -> Tuple[bool, bool, Optional[FVG]]:\n",
    "        \"\"\"Detect Fair Value Gap\"\"\"\n",
    "        if bar_idx < 2:\n",
    "            return False, False, None\n",
    "        \n",
    "        threshold = self.fvg_threshold_pct\n",
    "        if self.fvg_auto and bar_idx > 0:\n",
    "            # Auto threshold calculation\n",
    "            cum_ratio = ((df['high'] - df['low']) / df['low']).iloc[:bar_idx+1].sum()\n",
    "            threshold = cum_ratio / (bar_idx + 1) if bar_idx > 0 else 0\n",
    "        \n",
    "        current_low = df['low'].iloc[bar_idx]\n",
    "        current_high = df['high'].iloc[bar_idx]\n",
    "        prev_high = df['high'].iloc[bar_idx - 2]\n",
    "        prev_low = df['low'].iloc[bar_idx - 2]\n",
    "        prev_close = df['close'].iloc[bar_idx - 1]\n",
    "        \n",
    "        # Bullish FVG: low > high[2] and close[1] > high[2]\n",
    "        bull_fvg = (current_low > prev_high and \n",
    "                   prev_close > prev_high and \n",
    "                   (current_low - prev_high) / prev_high > threshold)\n",
    "        \n",
    "        # Bearish FVG: high < low[2] and close[1] < low[2]\n",
    "        bear_fvg = (current_high < prev_low and \n",
    "                   prev_close < prev_low and \n",
    "                   (prev_low - current_high) / current_high > threshold)\n",
    "        \n",
    "        new_fvg = None\n",
    "        if bull_fvg:\n",
    "            new_fvg = FVG(\n",
    "                max_price=current_low,\n",
    "                min_price=prev_high,\n",
    "                is_bullish=True,\n",
    "                timestamp=df.index[bar_idx],\n",
    "                bar_index=bar_idx\n",
    "            )\n",
    "        elif bear_fvg:\n",
    "            new_fvg = FVG(\n",
    "                max_price=prev_low,\n",
    "                min_price=current_high,\n",
    "                is_bullish=False,\n",
    "                timestamp=df.index[bar_idx],\n",
    "                bar_index=bar_idx\n",
    "            )\n",
    "        \n",
    "        return bull_fvg, bear_fvg, new_fvg\n",
    "    \n",
    "    def update_liquidity_levels(self, df: pd.DataFrame, bar_idx: int):\n",
    "        \"\"\"Update liquidity levels from daily high/low\"\"\"\n",
    "        if bar_idx == 0:\n",
    "            return\n",
    "        \n",
    "        current_date = df.index[bar_idx].date()\n",
    "        prev_date = df.index[bar_idx - 1].date() if bar_idx > 0 else None\n",
    "        \n",
    "        # Check if new day\n",
    "        is_new_day = prev_date is not None and current_date != prev_date\n",
    "        \n",
    "        if is_new_day:\n",
    "            self.yesterday_levels_added = False\n",
    "        \n",
    "        # Get previous day's high/low (simplified - in production, use daily timeframe)\n",
    "        if not self.yesterday_levels_added and bar_idx >= 1:\n",
    "            # For simplicity, use the highest high and lowest low of previous day\n",
    "            # In production, you'd query daily timeframe data\n",
    "            prev_day_data = df.iloc[max(0, bar_idx-288):bar_idx]  # Approximate previous day (288 bars for 5-min)\n",
    "            if len(prev_day_data) > 0:\n",
    "                yd_high = prev_day_data['high'].max()\n",
    "                yd_low = prev_day_data['low'].min()\n",
    "                \n",
    "                self.liquidity_levels.append(yd_high)\n",
    "                self.liquidity_levels.append(yd_low)\n",
    "                self.yesterday_levels_added = True\n",
    "                \n",
    "                # Maintain max levels\n",
    "                while len(self.liquidity_levels) > self.max_levels:\n",
    "                    self.liquidity_levels.pop(0)\n",
    "    \n",
    "    def find_nearest_liquidity_levels(self, price: float) -> Tuple[Optional[float], Optional[float]]:\n",
    "        \"\"\"Find nearest liquidity levels above and below price\"\"\"\n",
    "        if len(self.liquidity_levels) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        nearest_below = None\n",
    "        nearest_above = None\n",
    "        min_dist_below = float('inf')\n",
    "        min_dist_above = float('inf')\n",
    "        \n",
    "        for level in self.liquidity_levels:\n",
    "            if level < price:\n",
    "                dist = price - level\n",
    "                if dist < min_dist_below:\n",
    "                    min_dist_below = dist\n",
    "                    nearest_below = level\n",
    "            else:  # level >= price\n",
    "                dist = level - price\n",
    "                if dist < min_dist_above:\n",
    "                    min_dist_above = dist\n",
    "                    nearest_above = level\n",
    "        \n",
    "        return nearest_below, nearest_above\n",
    "    \n",
    "    def check_fvg_confirmation(self, df: pd.DataFrame, bar_idx: int, \n",
    "                              for_long: bool) -> bool:\n",
    "        \"\"\"Check if FVG confirmation exists for signal\"\"\"\n",
    "        if not self.require_fvg_confirmation:\n",
    "            return True\n",
    "        \n",
    "        current_price = float(df['close'].iloc[bar_idx])\n",
    "        proximity_distance = self.fvg_proximity_ticks * self.tick_size\n",
    "        \n",
    "        has_bull_fvg = False\n",
    "        has_bear_fvg = False\n",
    "        \n",
    "        # Check recent FVG first\n",
    "        if self.recent_bull_fvg and self.recent_bull_fvg_min is not None:\n",
    "            if abs(current_price - self.recent_bull_fvg_min) <= proximity_distance:\n",
    "                has_bull_fvg = True\n",
    "        \n",
    "        if self.recent_bear_fvg and self.recent_bear_fvg_max is not None:\n",
    "            if abs(current_price - self.recent_bear_fvg_max) <= proximity_distance:\n",
    "                has_bear_fvg = True\n",
    "        \n",
    "        # Check unmitigated FVGs\n",
    "        if not has_bull_fvg or not has_bear_fvg:\n",
    "            for fvg in self.fvg_records[:5]:  # Check up to 5 most recent\n",
    "                if fvg.is_bullish and not has_bull_fvg:\n",
    "                    if abs(current_price - fvg.min_price) <= proximity_distance:\n",
    "                        has_bull_fvg = True\n",
    "                elif not fvg.is_bullish and not has_bear_fvg:\n",
    "                    if abs(current_price - fvg.max_price) <= proximity_distance:\n",
    "                        has_bear_fvg = True\n",
    "        \n",
    "        # Apply direction filtering\n",
    "        if self.fvg_signal_type == \"Same Direction\":\n",
    "            return has_bull_fvg if for_long else has_bear_fvg\n",
    "        elif self.fvg_signal_type == \"Opposite Direction\":\n",
    "            return has_bear_fvg if for_long else has_bull_fvg\n",
    "        else:  # \"Any Direction\"\n",
    "            return has_bull_fvg or has_bear_fvg\n",
    "    \n",
    "    def check_inside_fvg(self, df: pd.DataFrame, bar_idx: int) -> Tuple[bool, bool]:\n",
    "        \"\"\"Check if price is inside an FVG\"\"\"\n",
    "        current_price = float(df['close'].iloc[bar_idx])\n",
    "        is_inside_bull = False\n",
    "        is_inside_bear = False\n",
    "        \n",
    "        if self.inside_fvg_signals:\n",
    "            for fvg in self.fvg_records[:self.inside_fvg_max_bars]:\n",
    "                if fvg.is_inside(current_price):\n",
    "                    if fvg.is_bullish:\n",
    "                        is_inside_bull = True\n",
    "                    else:\n",
    "                        is_inside_bear = True\n",
    "                    break\n",
    "        \n",
    "        return is_inside_bull, is_inside_bear\n",
    "    \n",
    "    def check_signals(self, df: pd.DataFrame, bar_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Check for trading signals\"\"\"\n",
    "        if bar_idx < max(50, self.fade_mom_lookback):\n",
    "            return {}\n",
    "        \n",
    "        signals = {\n",
    "            'long': False,\n",
    "            'short': False,\n",
    "            'signal_type': None\n",
    "        }\n",
    "        \n",
    "        # Calculate indicators\n",
    "        df_copy = df.iloc[:bar_idx+1].copy()\n",
    "        \n",
    "        # ATR - convert to float and handle NaN\n",
    "        atr_value = float(self.calculate_atr(df_copy, self.atr_period).iloc[-1])\n",
    "        if pd.isna(atr_value):\n",
    "            atr_value = 0.0\n",
    "        \n",
    "        # Volume MA - convert to float\n",
    "        avg_volume = float(df_copy['volume'].rolling(self.vol_lookback).mean().iloc[-1])\n",
    "        if pd.isna(avg_volume):\n",
    "            avg_volume = 0.0\n",
    "        \n",
    "        # ATR Volatility Filter\n",
    "        if self.use_atr_volatility_filter:\n",
    "            atr_vol = float(self.calculate_atr(df_copy, self.atr_vol_filter_period).iloc[-1])\n",
    "            avg_atr_vol = float(self.calculate_atr(df_copy, self.atr_vol_filter_period).rolling(\n",
    "                self.atr_vol_filter_ma_period).mean().iloc[-1])\n",
    "            \n",
    "            if pd.isna(atr_vol) or pd.isna(avg_atr_vol) or avg_atr_vol == 0:\n",
    "                return signals\n",
    "            \n",
    "            if not (atr_vol > avg_atr_vol * self.atr_vol_filter_min and\n",
    "                   atr_vol < avg_atr_vol * self.atr_vol_filter_max):\n",
    "                return signals\n",
    "        \n",
    "        # Volume Confirmation\n",
    "        if self.use_volume_confirmation:\n",
    "            vol_ma = float(df_copy['volume'].rolling(self.volume_filter_ma_period).mean().iloc[-1])\n",
    "            current_vol = float(df_copy['volume'].iloc[-1])\n",
    "            if pd.isna(vol_ma) or pd.isna(current_vol) or current_vol <= vol_ma * self.volume_filter_multiplier:\n",
    "                return signals\n",
    "        \n",
    "        # Detect momentum candle\n",
    "        # Convert to float to handle Decimal types from database\n",
    "        avg_candle_size = float((df_copy['high'] - df_copy['low']).rolling(\n",
    "            self.fade_mom_lookback).mean().iloc[-1])\n",
    "        current_candle_size = float(df_copy['high'].iloc[-1] - df_copy['low'].iloc[-1])\n",
    "        \n",
    "        # Handle NaN values\n",
    "        if pd.isna(avg_candle_size) or pd.isna(current_candle_size) or avg_candle_size <= 0:\n",
    "            is_momentum_candle = False\n",
    "        else:\n",
    "            is_momentum_candle = (current_candle_size / avg_candle_size >= self.fade_momentum_threshold)\n",
    "        \n",
    "        # Convert to float for comparison\n",
    "        current_close = float(df_copy['close'].iloc[-1])\n",
    "        current_open = float(df_copy['open'].iloc[-1])\n",
    "        current_candle_bullish = current_close > current_open\n",
    "        \n",
    "        if is_momentum_candle:\n",
    "            self.last_momentum_candle_bar = bar_idx\n",
    "            self.last_momentum_candle_bullish = current_candle_bullish\n",
    "        \n",
    "        # Check if momentum is recent enough\n",
    "        bars_since_momentum = (bar_idx - self.last_momentum_candle_bar \n",
    "                              if self.last_momentum_candle_bar is not None \n",
    "                              else self.fade_mom_lookback + 1)\n",
    "        \n",
    "        if self.last_momentum_candle_bar is not None:\n",
    "            time_diff = (df.index[bar_idx] - df.index[self.last_momentum_candle_bar]).total_seconds() / 60\n",
    "            fade_momentum_recent = time_diff <= self.fade_max_momentum_age_mins\n",
    "        else:\n",
    "            fade_momentum_recent = False\n",
    "        \n",
    "        # Volume confirmation - convert to float\n",
    "        current_vol = float(df_copy['volume'].iloc[-1])\n",
    "        volume_confirmation = (not self.require_volume_surge or \n",
    "                             current_vol > avg_volume * self.min_volume_ratio)\n",
    "        \n",
    "        # Find nearest liquidity levels\n",
    "        current_price = float(current_close)\n",
    "        nearest_below, nearest_above = self.find_nearest_liquidity_levels(current_price)\n",
    "        \n",
    "        # Check inside FVG signals first\n",
    "        is_inside_bull_fvg, is_inside_bear_fvg = self.check_inside_fvg(df, bar_idx)\n",
    "        \n",
    "        if self.inside_fvg_signals:\n",
    "            if is_inside_bull_fvg:\n",
    "                signals['long'] = True\n",
    "                signals['signal_type'] = 'Inside Bullish FVG'\n",
    "            if is_inside_bear_fvg:\n",
    "                signals['short'] = True\n",
    "                signals['signal_type'] = 'Inside Bearish FVG'\n",
    "        \n",
    "        # Check fade signals (only if not already triggered)\n",
    "        if not signals['long'] and not signals['short'] and fade_momentum_recent:\n",
    "            # Fade Long: Previous momentum was BEARISH, price near liquidity below\n",
    "            if (not self.last_momentum_candle_bullish and\n",
    "                nearest_below is not None and\n",
    "                (current_price - nearest_below) <= self.fade_proximity_ticks * self.tick_size and\n",
    "                current_price >= nearest_below and\n",
    "                volume_confirmation and\n",
    "                self.check_fvg_confirmation(df, bar_idx, for_long=True)):\n",
    "                signals['long'] = True\n",
    "                signals['signal_type'] = 'Fade Long'\n",
    "            \n",
    "            # Fade Short: Previous momentum was BULLISH, price near liquidity above\n",
    "            if (self.last_momentum_candle_bullish and\n",
    "                nearest_above is not None and\n",
    "                (nearest_above - current_price) <= self.fade_proximity_ticks * self.tick_size and\n",
    "                current_price <= nearest_above and\n",
    "                volume_confirmation and\n",
    "                self.check_fvg_confirmation(df, bar_idx, for_long=False)):\n",
    "                signals['short'] = True\n",
    "                signals['signal_type'] = 'Fade Short'\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def execute_trade(self, direction: str, price: float, atr_value: float, \n",
    "                     timestamp: pd.Timestamp, signal_type: str):\n",
    "        \"\"\"Execute a trade\"\"\"\n",
    "        # Validate inputs\n",
    "        if price is None or np.isnan(price) or price <= 0:\n",
    "            logger.warning(f\"Invalid price: {price}, skipping trade\")\n",
    "            return\n",
    "        \n",
    "        # Handle NaN ATR - use tick-based stops if ATR is invalid\n",
    "        atr_valid = atr_value is not None and not np.isnan(atr_value) and atr_value > 0\n",
    "        \n",
    "        if self.use_atr_exits and atr_valid:\n",
    "            if direction == 'long':\n",
    "                stop_loss = price - atr_value * self.atr_stop_loss_mult\n",
    "                take_profit = price + atr_value * self.atr_take_profit_mult\n",
    "            else:\n",
    "                stop_loss = price + atr_value * self.atr_stop_loss_mult\n",
    "                take_profit = price - atr_value * self.atr_take_profit_mult\n",
    "        else:\n",
    "            # Use tick-based stops\n",
    "            if direction == 'long':\n",
    "                stop_loss = price - self.stop_loss_ticks * self.tick_size\n",
    "                take_profit = price + self.take_profit_ticks * self.tick_size\n",
    "            else:\n",
    "                stop_loss = price + self.stop_loss_ticks * self.tick_size\n",
    "                take_profit = price - self.take_profit_ticks * self.tick_size\n",
    "        \n",
    "        # Ensure ATR is a valid float for database\n",
    "        atr_for_db = float(atr_value) if atr_valid else 0.0\n",
    "        \n",
    "        trade_data = {\n",
    "            'symbol': self.symbol,\n",
    "            'direction': direction,\n",
    "            'quantity': float(self.quantity),\n",
    "            'entry_price': float(price),\n",
    "            'stop_loss': float(stop_loss),\n",
    "            'take_profit': float(take_profit),\n",
    "            'atr_value': atr_for_db,\n",
    "            'daily_trade_number': len(self.backtest_results) + 1,\n",
    "            'mlflow_run_id': self.current_mlflow_run.info.run_id if self.current_mlflow_run else None,\n",
    "            'signal_type': signal_type\n",
    "        }\n",
    "        \n",
    "        trade_id = self.db.insert_trade(trade_data) if self.db else None\n",
    "        \n",
    "        self.backtest_results.append({\n",
    "            'direction': direction,\n",
    "            'entry_price': price,\n",
    "            'stop_loss': stop_loss,\n",
    "            'take_profit': take_profit,\n",
    "            'entry_time': timestamp,\n",
    "            'atr_value': atr_value,\n",
    "            'signal_type': signal_type,\n",
    "            'trade_id': trade_id\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"{signal_type}: {direction.upper()} @ ${price:.2f}, SL: ${stop_loss:.2f}, TP: ${take_profit:.2f}\")\n",
    "    \n",
    "    def calculate_metrics(self, df: pd.DataFrame):\n",
    "        \"\"\"Calculate backtest metrics\"\"\"\n",
    "        if len(self.backtest_results) == 0:\n",
    "            return {}\n",
    "        \n",
    "        returns = []\n",
    "        for trade in self.backtest_results:\n",
    "            entry_price = trade['entry_price']\n",
    "            entry_time = trade['entry_time']\n",
    "            direction = trade['direction']\n",
    "            stop_loss = trade['stop_loss']\n",
    "            take_profit = trade['take_profit']\n",
    "            \n",
    "            # Find entry index\n",
    "            try:\n",
    "                entry_idx = df.index.get_loc(entry_time)\n",
    "            except KeyError:\n",
    "                entry_idx = df.index.searchsorted(entry_time)\n",
    "                if entry_idx >= len(df):\n",
    "                    entry_idx = len(df) - 1\n",
    "            \n",
    "            # Find exit\n",
    "            exit_price = None\n",
    "            for j in range(entry_idx + 1, min(entry_idx + 100, len(df))):\n",
    "                bar = df.iloc[j]\n",
    "                \n",
    "                if direction == 'long':\n",
    "                    if bar['low'] <= stop_loss:\n",
    "                        exit_price = stop_loss\n",
    "                        break\n",
    "                    elif bar['high'] >= take_profit:\n",
    "                        exit_price = take_profit\n",
    "                        break\n",
    "                else:\n",
    "                    if bar['high'] >= stop_loss:\n",
    "                        exit_price = stop_loss\n",
    "                        break\n",
    "                    elif bar['low'] <= take_profit:\n",
    "                        exit_price = take_profit\n",
    "                        break\n",
    "            \n",
    "            if exit_price is None:\n",
    "                exit_price = df['close'].iloc[-1]\n",
    "            \n",
    "            # Calculate return\n",
    "            if direction == 'long':\n",
    "                trade_return = (exit_price - entry_price) / entry_price\n",
    "            else:\n",
    "                trade_return = (entry_price - exit_price) / entry_price\n",
    "            \n",
    "            returns.append(trade_return)\n",
    "        \n",
    "        if len(returns) == 0:\n",
    "            return {}\n",
    "        \n",
    "        returns_array = np.array(returns)\n",
    "        total_return = np.sum(returns_array)\n",
    "        total_trades = len(returns)\n",
    "        winning_trades = len([r for r in returns if r > 0])\n",
    "        losing_trades = len([r for r in returns if r < 0])\n",
    "        win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        avg_return = np.mean(returns_array)\n",
    "        std_return = np.std(returns_array)\n",
    "        \n",
    "        bars_per_day = 288  # 5-min bars\n",
    "        sharpe_ratio = (avg_return / std_return * np.sqrt(bars_per_day)) if std_return > 0 else 0\n",
    "        \n",
    "        cumulative_returns = np.cumsum(returns_array)\n",
    "        running_max = np.maximum.accumulate(cumulative_returns)\n",
    "        drawdown = cumulative_returns - running_max\n",
    "        max_drawdown = np.min(drawdown) if len(drawdown) > 0 else 0\n",
    "        \n",
    "        gross_profit = sum([r for r in returns if r > 0])\n",
    "        gross_loss = abs(sum([r for r in returns if r < 0]))\n",
    "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
    "        \n",
    "        avg_win = np.mean([r for r in returns if r > 0]) if winning_trades > 0 else 0\n",
    "        avg_loss = np.mean([r for r in returns if r < 0]) if losing_trades > 0 else 0\n",
    "        \n",
    "        final_equity = self.initial_capital * (1 + total_return)\n",
    "        \n",
    "        return {\n",
    "            'total_trades': total_trades,\n",
    "            'winning_trades': winning_trades,\n",
    "            'losing_trades': losing_trades,\n",
    "            'win_rate': win_rate,\n",
    "            'total_return_pct': total_return * 100,\n",
    "            'roi': (final_equity - self.initial_capital) / self.initial_capital * 100,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown_pct': max_drawdown * 100,\n",
    "            'profit_factor': profit_factor,\n",
    "            'avg_win_pct': avg_win * 100,\n",
    "            'avg_loss_pct': avg_loss * 100,\n",
    "            'initial_capital': self.initial_capital,\n",
    "            'final_equity': final_equity,\n",
    "            'net_profit': final_equity - self.initial_capital\n",
    "        }\n",
    "    \n",
    "    def run_backtest_from_db(self, start_date: str, end_date: str, timeframe: str = '5Min'):\n",
    "        \"\"\"Run backtest using data from TimescaleDB\"\"\"\n",
    "        logger.info(f\"Starting FVG Fade backtest from {start_date} to {end_date}\")\n",
    "        \n",
    "        df = self.data_collector.get_historical_data_from_db(\n",
    "            self.symbol, start_date, end_date, timeframe\n",
    "        )\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            logger.error(\"No data found in database\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Backtesting on {len(df)} bars\")\n",
    "        \n",
    "        # MLflow\n",
    "        mlflow.set_experiment(\"btc_backtest_fvg_fade\")\n",
    "        self.current_mlflow_run = mlflow.start_run()\n",
    "        \n",
    "        try:\n",
    "            mlflow.log_param(\"strategy\", \"FVG_Fade\")\n",
    "            mlflow.log_param(\"symbol\", self.symbol)\n",
    "            mlflow.log_param(\"start_date\", start_date)\n",
    "            mlflow.log_param(\"end_date\", end_date)\n",
    "            mlflow.log_param(\"timeframe\", timeframe)\n",
    "            \n",
    "            # Process each bar\n",
    "            for i in range(max(50, self.fade_mom_lookback), len(df)):\n",
    "                # Update liquidity levels\n",
    "                self.update_liquidity_levels(df, i)\n",
    "                \n",
    "                # Detect FVG\n",
    "                bull_fvg, bear_fvg, new_fvg = self.detect_fvg(df, i)\n",
    "                \n",
    "                if new_fvg is not None:\n",
    "                    # Check if FVG already exists (avoid duplicates)\n",
    "                    is_duplicate = False\n",
    "                    for existing_fvg in self.fvg_records:\n",
    "                        if (abs(existing_fvg.timestamp - new_fvg.timestamp).total_seconds() < 300 and\n",
    "                            existing_fvg.is_bullish == new_fvg.is_bullish):\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "                    \n",
    "                    if not is_duplicate:\n",
    "                        self.fvg_records.insert(0, new_fvg)\n",
    "                        \n",
    "                        # Update recent FVG tracking\n",
    "                        if bull_fvg:\n",
    "                            self.recent_bull_fvg = True\n",
    "                            self.recent_bear_fvg = False\n",
    "                            self.recent_bull_fvg_min = new_fvg.min_price\n",
    "                            self.recent_bull_fvg_max = new_fvg.max_price\n",
    "                            self.fvg_detection_bar = i\n",
    "                        elif bear_fvg:\n",
    "                            self.recent_bull_fvg = False\n",
    "                            self.recent_bear_fvg = True\n",
    "                            self.recent_bear_fvg_min = new_fvg.min_price\n",
    "                            self.recent_bear_fvg_max = new_fvg.max_price\n",
    "                            self.fvg_detection_bar = i\n",
    "                \n",
    "                # Reset recent FVG after 5 bars\n",
    "                if i - self.fvg_detection_bar > 5:\n",
    "                    self.recent_bull_fvg = False\n",
    "                    self.recent_bear_fvg = False\n",
    "                \n",
    "                # Remove mitigated FVGs\n",
    "                current_price = float(df['close'].iloc[i])\n",
    "                self.fvg_records = [fvg for fvg in self.fvg_records \n",
    "                                   if not fvg.is_mitigated(current_price)]\n",
    "                \n",
    "                # Check for signals\n",
    "                signals = self.check_signals(df, i)\n",
    "                \n",
    "                if signals.get('long') or signals.get('short'):\n",
    "                    # Calculate ATR and ensure it's a valid float\n",
    "                    atr_value = self.calculate_atr(df.iloc[:i+1], self.atr_period).iloc[-1]\n",
    "                    atr_value = float(atr_value) if not pd.isna(atr_value) else 0.0\n",
    "                    \n",
    "                    # Get price as float\n",
    "                    entry_price = float(df['close'].iloc[i])\n",
    "                    \n",
    "                    if signals.get('long'):\n",
    "                        self.execute_trade('long', entry_price, atr_value,\n",
    "                                         df.index[i], signals.get('signal_type', 'Long'))\n",
    "                        self.last_signal_bar = i\n",
    "                    \n",
    "                    if signals.get('short'):\n",
    "                        self.execute_trade('short', entry_price, atr_value,\n",
    "                                         df.index[i], signals.get('signal_type', 'Short'))\n",
    "                        self.last_signal_bar = i\n",
    "                \n",
    "                # Progress logging\n",
    "                if i % 1000 == 0:\n",
    "                    logger.info(f\"Backtest progress: {i}/{len(df)} bars ({i/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_metrics(df)\n",
    "            \n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(\"BACKTEST RESULTS\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            logger.info(f\"Total Trades: {metrics.get('total_trades', 0)}\")\n",
    "            logger.info(f\"Winning Trades: {metrics.get('winning_trades', 0)}\")\n",
    "            logger.info(f\"Losing Trades: {metrics.get('losing_trades', 0)}\")\n",
    "            logger.info(f\"Win Rate: {metrics.get('win_rate', 0)*100:.2f}%\")\n",
    "            logger.info(f\"Total Return: {metrics.get('total_return_pct', 0):.2f}%\")\n",
    "            logger.info(f\"ROI: {metrics.get('roi', 0):.2f}%\")\n",
    "            logger.info(f\"Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.4f}\")\n",
    "            logger.info(f\"Max Drawdown: {metrics.get('max_drawdown_pct', 0):.2f}%\")\n",
    "            logger.info(f\"Profit Factor: {metrics.get('profit_factor', 0):.4f}\")\n",
    "            logger.info(f\"{'='*60}\\n\")\n",
    "            \n",
    "            # Log to MLflow\n",
    "            for key, value in metrics.items():\n",
    "                mlflow.log_metric(key, value)\n",
    "            \n",
    "            # Store in database\n",
    "            if self.db:\n",
    "                metrics_for_db = metrics.copy()\n",
    "                metrics_for_db.update({\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'symbol': self.symbol,\n",
    "                    'timeframe': timeframe,\n",
    "                    'mlflow_run_id': self.current_mlflow_run.info.run_id if self.current_mlflow_run else None\n",
    "                })\n",
    "                self.db.insert_backtest_results(metrics_for_db)\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        finally:\n",
    "            mlflow.end_run()\n",
    "\n",
    "# Configuration\n",
    "class BacktestConfig:\n",
    "    def __init__(self):\n",
    "        self.ALPACA_API_KEY = os.getenv('ALPACA_API_KEY', 'PKSWFXHIT7WAESKFYXTTJ6DKUE')\n",
    "        self.ALPACA_API_SECRET = os.getenv('ALPACA_API_SECRET', 'A4nDUtAxdWijWjmg4zPVXcPeciaKhfkzwJ2wVF4gS5sg')\n",
    "        \n",
    "        self.TIMESCALEDB_URL = os.getenv(\n",
    "            'TIMESCALEDB_URL', \n",
    "            'postgresql://rayhan:12102801Rr@timescaledb:5432/arafatdb'\n",
    "        )\n",
    "        \n",
    "        self.MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000')\n",
    "        self.LOKI_URL = os.getenv('LOKI_URL', 'http://loki:3100/loki/api/v1/push')\n",
    "        \n",
    "        self.db = TimescaleDB(self.TIMESCALEDB_URL)\n",
    "        mlflow.set_tracking_uri(self.MLFLOW_TRACKING_URI)\n",
    "        \n",
    "        global logger\n",
    "        logger = setup_logging(\n",
    "            level=logging.INFO,\n",
    "            loki_url=self.LOKI_URL,\n",
    "            loki_labels={'service': 'btc_backtest_fvg'}\n",
    "        )\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    config = BacktestConfig()\n",
    "    \n",
    "    collector = HistoricalDataCollector(\n",
    "        db=config.db,\n",
    "        api_key=config.ALPACA_API_KEY,\n",
    "        api_secret=config.ALPACA_API_SECRET\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FVG Fade Strategy Backtest\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n  Make sure you've collected data first!\\n\")\n",
    "    \n",
    "    # Check if data exists\n",
    "    test_df = collector.get_historical_data_from_db(\n",
    "        'BTCUSD', '2021-01-01', '2022-12-31', '5Min'\n",
    "    )\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        logger.info(\"Trying with BTC/USD format...\")\n",
    "        test_df = collector.get_historical_data_from_db(\n",
    "            'BTC/USD', '2021-01-01', '2022-12-31', '5Min'\n",
    "        )\n",
    "    \n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        print(\" ERROR: No data found in database!\")\n",
    "        print(\"Please run data collection first using btcbacktest.py\")\n",
    "    else:\n",
    "        print(f\" Found {len(test_df)} bars in database. Running backtest...\\n\")\n",
    "        \n",
    "        # Create bot with strategy parameters\n",
    "        bot = FVGFadeBacktestBot(\n",
    "            db=config.db,\n",
    "            data_collector=collector,\n",
    "            symbol='BTCUSD',\n",
    "            quantity=0.001,\n",
    "            initial_capital=10000.0,\n",
    "            # FVG Settings\n",
    "            fvg_threshold_pct=0.0,\n",
    "            fvg_auto=False,\n",
    "            fvg_proximity_ticks=50,\n",
    "            # Fade Settings\n",
    "            fade_momentum_threshold=1.1,\n",
    "            fade_mom_lookback=20,\n",
    "            fade_max_momentum_age_mins=120,\n",
    "            fade_proximity_ticks=25,\n",
    "            # Trade Management\n",
    "            stop_loss_ticks=100,\n",
    "            take_profit_ticks=100,\n",
    "            use_atr_exits=False,\n",
    "            atr_period=14,\n",
    "            atr_stop_loss_mult=1.5,\n",
    "            atr_take_profit_mult=3.0,\n",
    "            # Filters\n",
    "            require_volume_surge=False,\n",
    "            vol_lookback=20,\n",
    "            min_volume_ratio=1.2,\n",
    "            use_atr_volatility_filter=False,\n",
    "            use_volume_confirmation=False,\n",
    "            # FVG Signal Requirements\n",
    "            require_fvg_confirmation=True,\n",
    "            fvg_signal_type=\"Same Direction\",\n",
    "            inside_fvg_signals=True,\n",
    "            inside_fvg_max_bars=20,\n",
    "            # Liquidity Levels\n",
    "            keep_levels_days=5,\n",
    "            tick_size=0.01  # Adjust for BTC\n",
    "        )\n",
    "        \n",
    "        # Run backtest\n",
    "        metrics = bot.run_backtest_from_db(\n",
    "            start_date='2021-01-01',\n",
    "            end_date='2022-12-31',\n",
    "            timeframe='5Min'\n",
    "        )\n",
    "        \n",
    "        if metrics:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"BACKTEST SUMMARY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Total Trades: {metrics.get('total_trades', 0)}\")\n",
    "            print(f\"Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.4f}\")\n",
    "            print(f\"Total Return: {metrics.get('total_return_pct', 0):.2f}%\")\n",
    "            print(f\"ROI: {metrics.get('roi', 0):.2f}%\")\n",
    "            print(f\"Max Drawdown: {metrics.get('max_drawdown_pct', 0):.2f}%\")\n",
    "            print(f\"Profit Factor: {metrics.get('profit_factor', 0):.4f}\")\n",
    "            print(f\"Win Rate: {metrics.get('win_rate', 0)*100:.2f}%\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144f45d-9d15-41ee-8bd1-591558aa014e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3838c1-2be0-4849-b00f-d173d15ffbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
